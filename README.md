# Pinterest Data Pipeline

## Table of Contents
- Project Brief
- Project Data
- Project Tools
- Milestone 3
- Milestone 4
- Milestone 5
- Milestone 6
- Milestone 7
- Milestone 8
- Milestone 9

## Project Brief
Build the system that Pinterest uses to analyse both historical, and real-time data generated by post from their users.

Pinterest has world-class machine learning engineering systems. They have billioons of user interactions such as image uploads or image clicks which they need to process every day to inform the decisions to make. In this project, I am building a system in the cloud that takes in those events and runs them throught two separate pipelines. One for computing real-time metrics such as profile popularity, which would be used to recommend that profile in real-time. Another is for computing metrics that depend on historical data such as the most popular category this year.

## Milestone 3: Configure the EC2 Kafka Client
Goal: 
- Create 3 topics with MSK

Tasks:
1. Creating a .pem key
    - Navigating to AWS Parameter Store to find the key pair value and saving it as "Key pair name.pem" file.

2. Connecting to EC2
    - Connecting my local device into AWS EC2 by running the code below inside a terminal:
    ```
    ssh -i "12f4a3e5b9c5-key-pair.pem" ec2-user@ec2-184-73-115-68.compute-1.amazonaws.com
    ```

3. Set up Kafka in EC2 instance
    - Once the EC2 was connected, Kafka was installed and client.properties file was created to configure Kafka Client to use AWS IAM authentication to the cluster.

    client.properties file contains:
    ```
    # Sets up TLS for encryption and SASL for authN.
    security.protocol = SASL_SSL

    # Identifies the SASL mechanism to use.
    sasl.mechanism = AWS_MSK_IAM

    # Binds SASL client implementation.
    sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn="arn:aws:iam::584739742957:role/12f4a3e5b9c5-ec2-access-role";

    # Encapsulates constructing a SigV4 signature based on extracted credentials.
    # The SASL client bound by "sasl.jaas.config" invokes this class.
    sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler
    ```

4. Creating Kafka Topics
    - Before a topic was created, I had to grabbed my MSK cluster specific Bootstrap servers string and also its Plaintext Apache Zookeeper connection string.
    - Using the correct strings, I managed to create 3 topics:
        - 12f4a3e5b9c5.pin
        - 12f4a3e5b9c5.geo
        - 12f4a3e5b9c5.user
    - The topics were created using this code below:
    ```
    ./kafka-topics.sh --create --zookeeper "PLAINTEXT://b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098
    " --replication-factor 2 --partitions 1 --topic 12f4a3e5b9c5.pin
    ```

## Milestone 4: Connect a MSK cluster to a S3 bucket
Goal:
- Use MSK Connect to connect the MSK cluster to a S3 bucket, such that any data going through the cluster will be automatically saved and stored in a dedicated S3 bucket.

Tasks:
1. Creating a custom plugin with MSK Connect
    - Before a custom plugin can be created inside MSK, Confluent.io Amazon S3 connector needs to be downloaded inside my EC2 client.
    ```
    wget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.0.3/confluentinc-kafka-connect-s3-10.0.3.zip
    ```
    - Inside the MSK Connect console, a custom plugin can be then set with the Confluent connector ZIP file inside a bucket

2. Creating a connector with MSK Connect
    - Inside MSK console, the connector configuration settings has been tweaked so it links to my own UUID and a specific S3 bucket.
    ```
    connector.class=io.confluent.connect.s3.S3SinkConnector
    # same region as our bucket and cluster
    s3.region=us-east-1
    flush.size=1
    schema.compatibility=NONE
    tasks.max=3
    # include nomeclature of topic name, given here as an example will read all data from topic names starting with msk.topic....
    topics.regex=<YOUR_UUID>.*
    format.class=io.confluent.connect.s3.format.json.JsonFormat
    partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner
    value.converter.schemas.enable=false
    value.converter=org.apache.kafka.connect.json.JsonConverter
    storage.class=io.confluent.connect.s3.storage.S3Storage
    key.converter=org.apache.kafka.connect.storage.StringConverter
    s3.bucket.name=<BUCKET_NAME>
    ```

## Milestone 5: Configuring an API Gateway
Goal: 
- To build our own API that send data to the MSK cluter, which in turn will be stored in an S3 bucket.

Tasks:
1. Build a Kafka REST Proxy
    - Inside AWS API Gateway, a resource is created that allows a PROXY integration to my API.
    - This PROXY then contains a HTTP ANY method, that has my EC2 PublicDNS as its Endpoint URL

2. Set up the REST Proxy inside EC2 client
    - Before the REST Proxy can be run inside the EC2 client, a Confluent package needs to be installed inside the client first.
    ```
    sudo wget https://packages.confluent.io/archive/7.2/confluent-7.2.0.tar.gz
    ```
    - Inside the confluent/etc/kafka-rest folder, a kafka-rest.properties file is editted to perform IAM authentication to the MSK cluster
    ```
    client.security.protocol = SASL_SSL

    # Identifies the SASL mechanism to use.
    client.sasl.mechanism = AWS_MSK_IAM

    # Binds SASL client implementation.
    client.sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn="arn:aws:iam::584739742957:role/12f4a3e5b9c5-ec2-access-role";

    # Encapsulates constructing a SigV4 signature based on extracted credentials.
    # The SASL client bound by "sasl.jaas.config" invokes this class.
    client.sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler
    ```
    - The REST proxy can be then run inside the EC2 client with the code below:
    ```
    # This code needs to be run inside confluent/bin folder
    ./kafka-rest-start /home/ec2-user/confluent-7.2.0/etc/kafka-rest/kafka-rest.properties
    ```

3. Send data to the API
    - Data is sent into the API by tweaking the user_posting_emulation.py
    ```
    #This function is called after every pin, geo, and user data is collected
    def send_data_to_Kafka(data, topic, payload):
    invoke_url = "https://iij0y5sisb.execute-api.us-east-1.amazonaws.com/Init"
    url = invoke_url + "/topics/" + topic
    print (url)

    headers = {'Content-Type': 'application/vnd.kafka.json.v2+json'}
    response = requests.request("POST",url, headers = headers, data = payload)
    print (response.status_code)

    return 

    #Data for pin
    for row in pin_selected_row:
                pin_result = dict(row._mapping)
                topic_name = "12f4a3e5b9c5.pin"
                payload = json.dumps({
                "records": [
                    {   
                    "value": {"index": pin_result["index"], 
                            "unique_id": pin_result["unique_id"], 
                            "title": pin_result["title"], 
                            "description": pin_result["description"], 
                            "poster_name": pin_result["poster_name"], 
                            "follower_count": pin_result["follower_count"], 
                            "tag_list": pin_result["tag_list"], 
                            "is_image_or_video": pin_result["is_image_or_video"], 
                            "image_src": pin_result["image_src"], 
                            "downloaded": pin_result["downloaded"], 
                            "save_location": pin_result["save_location"], 
                            "category": pin_result["category"]}
                        }
                    ]
                })
                send_data_to_Kafka(pin_result, topic_name, payload)
                print ("pin result sent")
    ```

## Milestone 6: Set-up Databricks
Goal:
- To mount S3 bucket inside Databricks

Tasks:
- Mount S3 bucket into Databricks
    - First we need to read the CSV file that contains our AWS keys inside Databricks.
    ```
    # pyspark functions
    from pyspark.sql.functions import *
    # URL processing
    import urllib

    # Specify file type to be csv
    file_type = "csv"
    # Indicates file has first row as the header
    first_row_is_header = "true"
    # Indicates file has comma as the delimeter
    delimiter = ","
    # Read the CSV file to spark dataframe
    aws_keys_df = spark.read.format(file_type)\
    .option("header", first_row_is_header)\
    .option("sep", delimiter)\
    .load("/FileStore/tables/authentication_credentials.csv")
    ```
    - Access key and Secret Access Key is required to mount a S3 bucket into Databricks. This can be extracted with this following code:
    ```
    # Get the AWS access key and secret key from the spark dataframe
    ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']
    SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']
    # Encode the secrete key
    ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe="")
    ```
    - With the Access Key and Encoded Secret Key, a S3 bucket can be then mounted into Databricks with this code:
    ```
    AWS_S3_BUCKET = "user-12f4a3e5b9c5-bucket"
    # Mount name for the bucket
    MOUNT_NAME = "/mnt/12f4a3e5b9c5-bucket"
    # Source url
    SOURCE_URL = "s3n://{0}:{1}@{2}".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)

    dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)
    ```

- Create a dataframe for each topic
    - A dataframe for each topic is created by finding its path, settings its file type, and reading it with this code below:
    ```    
    pin_file_location = "/mnt/12f4a3e5b9c5-bucket/topics/12f4a3e5b9c5.pin/partition=0/12f4a3e5b9c5.pin+0+0000000000.json" 
    geo_file_location = "/mnt/12f4a3e5b9c5-bucket/topics/12f4a3e5b9c5.geo/partition=0/12f4a3e5b9c5.geo+0+0000000000.json" 
    user_file_location = "/mnt/12f4a3e5b9c5-bucket/topics/12f4a3e5b9c5.user/partition=0/12f4a3e5b9c5.user+0+0000000000.json"

    file_type = "json"
    infer_schema = "true"

    df_pin = spark.read.format(file_type).option("inferSchema", infer_schema).load(pin_file_location)
    df_geo = spark.read.format(file_type).option("inferSchema", infer_schema).load(geo_file_location)
    df_user = spark.read.format(file_type).option("inferSchema", infer_schema).load(user_file_location)

    display(df_pin)
    display(df_geo)
    display(df_user)
    ```

## Milestone 7: Spark usage in Databricks
Goal:
- To clean and perform analyses to the data

Tasks:
- Clean each dataframe
- Query the data
    - Find the most popular category in each country
    - Find which was the most popular category each year
    - Find the user with the most followers in each country
    - Find the most popular category for different age groups
    - Find the median follower count for different age groups
    - Find how many users have joined each year
    - Find the median follower count of user based on their joining year
    - Find the median follower count of used based on their joining year and age group

## Milestone 8: AWS MWAA
Goal:
- Orchestrate Databricks workloads on AWS MWAA

Tasks
- Create and upload a DAG file into MWAA environment
- Manually trigger the DAG to run a Databricks Notebook

## Milestone 9: AWS Kinesis
Goal:
- Send streaming data to Kinesis and read this data inside Databricks

Tasks
- Create data stream using AWS Kinesis
- Configure API with Kinesis proxy integration
- Send data into Kinesis streams
- Read data into Databricks
- Transform data
- Write the data to Delta Tables