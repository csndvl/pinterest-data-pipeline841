# Pinterest Data Pipeline

## Table of Contents
- Project Brief
- Project Data
- Project Tools
- Milestone 3
- Milestone 4
- Milestone 5
- Milestone 6
- Milestone 7
- Milestone 8
- Milestone 9

## Project Brief
Build the system that Pinterest uses to analyse both historical, and real-time data generated by post from their users.

Pinterest has world-class machine learning engineering systems. They have billioons of user interactions such as image uploads or image clicks which they need to process every day to inform the decisions to make. In this project, I am building a system in the cloud that takes in those events and runs them throught two separate pipelines. One for computing real-time metrics such as profile popularity, which would be used to recommend that profile in real-time. Another is for computing metrics that depend on historical data such as the most popular category this year.

## Milestone 3: Configure the EC2 Kafka Client
Goal: 
- Create 3 topics with MSK

Tasks:
1. Creating a .pem key
    - Navigating to AWS Parameter Store to find the key pair value and saving it as "Key pair name.pem" file.

2. Connecting to EC2
    - Connecting my local device into AWS EC2 by running the code below inside a terminal:
    ```
    ssh -i "12f4a3e5b9c5-key-pair.pem" ec2-user@ec2-184-73-115-68.compute-1.amazonaws.com
    ```

3. Set up Kafka in EC2 instance
    - Once the EC2 was connected, Kafka was installed and client.properties file was created to configure Kafka Client to use AWS IAM authentication to the cluster.

    client.properties file contains:
    ```
    # Sets up TLS for encryption and SASL for authN.
    security.protocol = SASL_SSL

    # Identifies the SASL mechanism to use.
    sasl.mechanism = AWS_MSK_IAM

    # Binds SASL client implementation.
    sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn="arn:aws:iam::584739742957:role/12f4a3e5b9c5-ec2-access-role";

    # Encapsulates constructing a SigV4 signature based on extracted credentials.
    # The SASL client bound by "sasl.jaas.config" invokes this class.
    sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler
    ```

4. Creating Kafka Topics
    - Before a topic was created, I had to grabbed my MSK cluster specific Bootstrap servers string and also its Plaintext Apache Zookeeper connection string.
    - Using the correct strings, I managed to create 3 topics:
        - 12f4a3e5b9c5.pin
        - 12f4a3e5b9c5.geo
        - 12f4a3e5b9c5.user
    - The topics were created using this code below:
    ```
    ./kafka-topics.sh --create --zookeeper "PLAINTEXT://b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098
    " --replication-factor 2 --partitions 1 --topic 12f4a3e5b9c5.pin
    ```

## Milestone 4: Connect a MSK cluster to a S3 bucket
Goal:
- Use MSK Connect to connect the MSK cluster to a S3 bucket, such that any data going through the cluster will be automatically saved and stored in a dedicated S3 bucket.

Tasks:
1. Creating a custom plugin with MSK Connect
    - Before a custom plugin can be created inside MSK, Confluent.io Amazon S3 connector needs to be downloaded inside my EC2 client.
    ```
    wget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.0.3/confluentinc-kafka-connect-s3-10.0.3.zip
    ```
    - Inside the MSK Connect console, a custom plugin can be then set with the Confluent connector ZIP file inside a bucket

2. Creating a connector with MSK Connect
    - Inside MSK console, the connector configuration settings has been tweaked so it links to my own UUID and a specific S3 bucket.
    ```
    connector.class=io.confluent.connect.s3.S3SinkConnector
    # same region as our bucket and cluster
    s3.region=us-east-1
    flush.size=1
    schema.compatibility=NONE
    tasks.max=3
    # include nomeclature of topic name, given here as an example will read all data from topic names starting with msk.topic....
    topics.regex=<YOUR_UUID>.*
    format.class=io.confluent.connect.s3.format.json.JsonFormat
    partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner
    value.converter.schemas.enable=false
    value.converter=org.apache.kafka.connect.json.JsonConverter
    storage.class=io.confluent.connect.s3.storage.S3Storage
    key.converter=org.apache.kafka.connect.storage.StringConverter
    s3.bucket.name=<BUCKET_NAME>
    ```

## Milestone 5: Configuring an API Gateway
Goal: 
- To build our own API that send data to the MSK cluter, which in turn will be stored in an S3 bucket.

Tasks:
1. Build a Kafka REST Proxy
    - Inside AWS API Gateway, a resource is created that allows a PROXY integration to my API.
    - This PROXY then contains a HTTP ANY method, that has my EC2 PublicDNS as its Endpoint URL

2. Set up the REST Proxy inside EC2 client
    - Before the REST Proxy can be run inside the EC2 client, a Confluent package needs to be installed inside the client first.
    ```
    sudo wget https://packages.confluent.io/archive/7.2/confluent-7.2.0.tar.gz
    ```
    - Inside the confluent/etc/kafka-rest folder, a kafka-rest.properties file is editted to perform IAM authentication to the MSK cluster
    ```
    client.security.protocol = SASL_SSL

    # Identifies the SASL mechanism to use.
    client.sasl.mechanism = AWS_MSK_IAM

    # Binds SASL client implementation.
    client.sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn="arn:aws:iam::584739742957:role/12f4a3e5b9c5-ec2-access-role";

    # Encapsulates constructing a SigV4 signature based on extracted credentials.
    # The SASL client bound by "sasl.jaas.config" invokes this class.
    client.sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler
    ```
    - The REST proxy can be then run inside the EC2 client with the code below:
    ```
    # This code needs to be run inside confluent/bin folder
    ./kafka-rest-start /home/ec2-user/confluent-7.2.0/etc/kafka-rest/kafka-rest.properties
    ```

3. Send data to the API
    - Data is sent into the API by tweaking the user_posting_emulation.py
    ```
    #This function is called after every pin, geo, and user data is collected
    def send_data_to_Kafka(data, topic, payload):
    invoke_url = "https://iij0y5sisb.execute-api.us-east-1.amazonaws.com/Init"
    url = invoke_url + "/topics/" + topic
    print (url)

    headers = {'Content-Type': 'application/vnd.kafka.json.v2+json'}
    response = requests.request("POST",url, headers = headers, data = payload)
    print (response.status_code)

    return 

    #Data for pin
    for row in pin_selected_row:
                pin_result = dict(row._mapping)
                topic_name = "12f4a3e5b9c5.pin"
                payload = json.dumps({
                "records": [
                    {   
                    "value": {"index": pin_result["index"], 
                            "unique_id": pin_result["unique_id"], 
                            "title": pin_result["title"], 
                            "description": pin_result["description"], 
                            "poster_name": pin_result["poster_name"], 
                            "follower_count": pin_result["follower_count"], 
                            "tag_list": pin_result["tag_list"], 
                            "is_image_or_video": pin_result["is_image_or_video"], 
                            "image_src": pin_result["image_src"], 
                            "downloaded": pin_result["downloaded"], 
                            "save_location": pin_result["save_location"], 
                            "category": pin_result["category"]}
                        }
                    ]
                })
                send_data_to_Kafka(pin_result, topic_name, payload)
                print ("pin result sent")
    ```

## Milestone 6: Set-up Databricks
Goal:
- To mount S3 bucket inside Databricks

Tasks:
1. Mount S3 bucket into Databricks
    - First we need to read the CSV file that contains our AWS keys inside Databricks.
    ```
    # pyspark functions
    from pyspark.sql.functions import *
    # URL processing
    import urllib

    # Specify file type to be csv
    file_type = "csv"
    # Indicates file has first row as the header
    first_row_is_header = "true"
    # Indicates file has comma as the delimeter
    delimiter = ","
    # Read the CSV file to spark dataframe
    aws_keys_df = spark.read.format(file_type)\
    .option("header", first_row_is_header)\
    .option("sep", delimiter)\
    .load("/FileStore/tables/authentication_credentials.csv")
    ```
    - Access key and Secret Access Key is required to mount a S3 bucket into Databricks. This can be extracted with this following code:
    ```
    # Get the AWS access key and secret key from the spark dataframe
    ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']
    SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']
    # Encode the secrete key
    ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe="")
    ```
    - With the Access Key and Encoded Secret Key, a S3 bucket can be then mounted into Databricks with this code:
    ```
    AWS_S3_BUCKET = "user-12f4a3e5b9c5-bucket"
    # Mount name for the bucket
    MOUNT_NAME = "/mnt/12f4a3e5b9c5-bucket"
    # Source url
    SOURCE_URL = "s3n://{0}:{1}@{2}".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)

    dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)
    ```

2. Create a dataframe for each topic
    - A dataframe for each topic is created by finding its path, settings its file type, and reading it with this code below:
    ```    
    pin_file_location = "/mnt/12f4a3e5b9c5-bucket/topics/12f4a3e5b9c5.pin/partition=0/12f4a3e5b9c5.pin+0+0000000000.json" 
    geo_file_location = "/mnt/12f4a3e5b9c5-bucket/topics/12f4a3e5b9c5.geo/partition=0/12f4a3e5b9c5.geo+0+0000000000.json" 
    user_file_location = "/mnt/12f4a3e5b9c5-bucket/topics/12f4a3e5b9c5.user/partition=0/12f4a3e5b9c5.user+0+0000000000.json"

    file_type = "json"
    infer_schema = "true"

    df_pin = spark.read.format(file_type).option("inferSchema", infer_schema).load(pin_file_location)
    df_geo = spark.read.format(file_type).option("inferSchema", infer_schema).load(geo_file_location)
    df_user = spark.read.format(file_type).option("inferSchema", infer_schema).load(user_file_location)

    display(df_pin)
    display(df_geo)
    display(df_user)
    ```

## Milestone 7: Spark usage in Databricks
Goal:
- To clean and perform analyses to the data

Tasks:
1. Clean each dataframe
    - All three dataframes were cleaned by removing duplicates
    ```
    #Remove duplicated data
    df_pin = df_pin.drop_duplicates([column_name for column_name, data_type in df_pin.dtypes])
    df_pin.count()
    df_geo = df_geo.drop_duplicates([column_name for column_name, data_type in df_geo.dtypes])
    df_geo.count()
    df_user = df_user.drop_duplicates([column_name for column_name, data_type in df_user.dtypes])
    df_user.count()
    ```

    - Cleaning df_pin dataframe
    ```
    # Replace null values into None
    df_pin_clean = df_pin.withColumn("category", when(df_pin["category"].isNull(), None).otherwise(df_pin["category"]))
    df_pin_clean = df_pin.withColumn("description", when(df_pin["description"].isNull(), None).otherwise(df_pin["description"]))

    # Replace entries with no relevant data into None
    df_pin_clean = df_pin.withColumn("description", when(col("description").contains("No description"), None).otherwise(col("description")))
    df_pin_clean = df_pin.withColumn("follower_count", when(col("follower_count").contains("User Info Error"), None).otherwise(col("follower_count")))
    df_pin_clean = df_pin.withColumn("image_src", when(col("image_src").contains("Image src error"), None).otherwise(col("image_src")))
    df_pin_clean = df_pin.withColumn("poster_name", when(col("poster_name").contains("User Info Error"), None).otherwise(col("poster_name")))

    # Change M and k inside follower_column into its coresponding value
    df_pin_clean = df_pin_clean.withColumn("follower_count", regexp_replace(df_pin_clean["follower_count"], "M", "000000"))
    df_pin_clean = df_pin_clean.withColumn("follower_count", regexp_replace(df_pin_clean["follower_count"], "k", "000"))

    # Change follower_count data type into int
    df_pin_clean = df_pin_clean.withColumn("follower_count", df_pin_clean["follower_count"].cast("int"))

    # Ensuring that each column containing numeric data has a numeric data type
    df_pin_clean = df_pin_clean.withColumn("downloaded", df_pin_clean["downloaded"].cast("int"))
    df_pin_clean = df_pin_clean.withColumn("index", df_pin_clean["index"].cast("int"))

    # Cleaning the save_location column
    df_pin_clean = df_pin_clean.withColumn("save_location", regexp_replace(df_pin_clean["save_location"], "Local save in", ""))

    # Renaming index column into ind column
    df_pin_clean = df_pin_clean.withColumnRenamed("index", "ind")

    # Reorder dataframe columns
    df_pin_clean = df_pin_clean.select(["ind", "unique_id", "title", "description", "follower_count", "poster_name", "tag_list", "is_image_or_video", "image_src", "save_location", "category"])
    ```

    - Cleaning df_geo dataframe
    ```
    # Creating a new coordinates column
    df_geo_clean = df_geo.withColumn("coordinates", array(col("latitude"), col("longitude")))

    # Dropping latitude and longitude column
    df_geo_clean = df_geo_clean.drop("latitude", "longitude")

    # Coverting timestamp into a timestamp data type
    df_geo_clean = df_geo_clean.withColumn("timestamp", df_geo_clean["timestamp"].cast("timestamp"))

    # Reordering columns
    df_geo_clean = df_geo_clean.select(["ind", "country", "coordinates", "timestamp"])
    ```

    - Cleaning df_user dataframe
    ```
    # Creating an username column
    df_user_clean = df_user.withColumn("user_name", concat(col("first_name"), col("last_name")))

    # Dropping first and last name columns
    df_user_clean = df_user_clean.drop("first_name", "last_name")

    # Coverting date_joined column into a timestamp
    df_user_clean = df_user_clean.withColumn("date_joined", df_user_clean["date_joined"].cast("timestamp"))

    # Reodering columns
    df_user_clean = df_user_clean.select(["ind","user_name","age","date_joined"])
    ```

2. Query the data
    - Before querying the data, I created a temporary view in Spark SQL with the specified name "pin", "geo", and "user" to make typing the query a lot easier. This is done by running the code below:
    ```
    df_pin_clean.createOrReplaceTempView("pin")
    df_geo_clean.createOrReplaceTempView("geo")
    df_user_clean.createOrReplaceTempView("user")
    ```
    - Find the most popular category in each country
    ```
    result = spark.sql("""SELECT country, category, category_count
                    FROM (
                        SELECT country, category, COUNT(*) AS category_count,
                            ROW_NUMBER() OVER (PARTITION BY country ORDER BY COUNT(*) DESC) AS rank
                        FROM geo 
                        JOIN pin 
                        ON pin.ind = geo.ind
                        GROUP BY country, category
                    ) 
                    WHERE rank = 1
                    ORDER BY category_count DESC""")

    display (result)
    ```

    - Find which was the most popular category each year
    ```
    result = spark.sql("""SELECT post_year, category, category_count
                   FROM (
                       SELECT YEAR(timestamp) as post_year, category, COUNT(*) as category_count,
                           ROW_NUMBER() OVER (PARTITION BY YEAR(timestamp) ORDER BY COUNT(*) DESC) AS rank
                        FROM geo
                        JOIN pin
                        ON pin.ind = geo.ind
                        GROUP BY YEAR(timestamp), category
                   )
                   WHERE rank = 1
                   ORDER BY category_count DESC""")

    display (result)
    ```

    - Find the user with the most followers in each country
    ```
    result = spark.sql("""SELECT country, poster_name, MAX(follower_count) as follower_count
                   FROM pin
                   JOIn geo
                   ON geo.ind = pin.ind
                   GROUP BY country, poster_name
                   ORDER BY follower_count DESC""")

    display(result)
    ```

    - Find the country with the most follower
    ```
    result = spark.sql("""SELECT country, MAX(follower_count) as follower_count
                   FROM pin
                   JOIn geo
                   ON geo.ind = pin.ind
                   GROUP BY country
                   ORDER BY follower_count DESC
                   LIMIT 1""")

    display(result)
    ```

    - Find the most popular category per age groups
    ```
    result = spark.sql("""
    SELECT age_group, category, category_count
    FROM (
        SELECT age_group, category, category_count,
            ROW_NUMBER() OVER (PARTITION BY age_group ORDER BY category_count DESC) AS rank
        FROM (
            SELECT
                CASE
                    WHEN age BETWEEN 18 AND 24 THEN "18-24"
                    WHEN age BETWEEN 25 AND 35 THEN "25-35"
                    WHEN age BETWEEN 36 AND 50 THEN "36-50"
                    WHEN age > 50 THEN "50+"
                    ELSE "NONE"
                END AS age_group, category, COUNT(*) AS category_count
            FROM user
            JOIN pin
            ON pin.ind = user.ind
            GROUP BY age_group, category
            ORDER BY category_count DESC
        )
    )WHERE rank = 1 
    """)

    display(result)
    ```

    - Find the median follower count for each age group
    ```
    results = spark.sql("""
                    SELECT CASE 
                    WHEN age BETWEEN 18 AND 24 THEN "18-24"
                    WHEN age BETWEEN 25 AND 35 THEN "25-35"
                    WHEN age BETWEEN 36 AND 50 THEN "36-50"
                    WHEN age > 50 THEN "+50"
                    ELSE "NONE" 
                    END as age_group, PERCENTILE_CONT(0.5) WITHIN GROUP(ORDER BY follower_count) AS median_follower_count
                    FROM user
                    JOIN pin
                    ON pin.ind = user.ind
                    GROUP BY age_group
                    ORDER BY median_follower_count DESC
                    """)
    display(results)    
    ```

    - Find the number of users each year from 2015 - 2020
    ```
    results = spark.sql("""
                    SELECT year(date_joined) as join_year, COUNT(date_joined) as numbers_users_joined
                    FROM user 
                    GROUP by join_year
                    HAVING join_year BETWEEN 2015 and 2020
                    """)

    display(results)
    ```

    - Find the median follower count of users based on joining year
    ```
    results = spark.sql("""
                    SELECT year(date_joined) as join_year, PERCENTILE_CONT(0.5) WITHIN GROUP(ORDER BY follower_count) AS median_follower_count
                    FROM user 
                    JOIN pin
                    ON pin.ind = user.ind
                    GROUP by join_year
                    HAVING join_year BETWEEN 2015 and 2020
                    """)

    display(results)
    ```

    - Find the median follower count of users based on joining year and age group
    ```
    results = spark.sql("""
                    SELECT CASE
                    WHEN age BETWEEN 18 AND 24 THEN "18-24"
                    WHEN age BETWEEN 25 AND 35 THEN "25-35"
                    WHEN age BETWEEN 36 AND 50 THEN "36-50"
                    WHEN age > 50 THEN "+50"
                    ELSE "NONE" 
                    END as age_group, year(date_joined) as join_year, PERCENTILE_CONT(0.5) WITHIN GROUP(ORDER BY follower_count) AS median_follower_count
                    FROM user 
                    JOIN pin
                    ON pin.ind = user.ind
                    GROUP by join_year, age_group
                    HAVING join_year BETWEEN 2015 and 2020
                    ORDER BY join_year ASC, median_follower_count DESC
                    """)

    display(results)
    ```

## Milestone 8: AWS MWAA
Goal:
- Orchestrate Databricks workloads on AWS MWAA

Tasks
1. Create and upload a DAG file into MWAA environment
    - DAG created: 12f4a3e5b9c5_dag.py
    - This is uploaded inside S3 bucket name "mwaa-dag-bucket"
2. Manually trigger the DAG to run a Databricks Notebook
    - Inside MWAA, the DAG is trigger to check if it runs successfully

## Milestone 9: AWS Kinesis
Goal:
- Send streaming data to Kinesis and read this data inside Databricks

Tasks
1. Create data stream using AWS Kinesis
    - Inside AWS Kinesis, 3 data streams were made
        1. streaming-12f4a3e5b9c5-pin
        2. streaming-12f4a3e5b9c5-geo
        3. streaming-12f4a3e5b9c5-user

2. Configure API with Kinesis proxy integration
    - A new resource is made inside AWS API Gateway to integrate Kinesis 
        - Added HTTP Headers 
        - Added Mapping Templates

3. Send data into Kinesis streams
    - A new file named user_posting_emulation_streaming.py is created to send data into Kinesis stream.
    - After converting each data into the correct data type, this code is run to send it into the API.
    ```
     headers = {'Content-Type': 'application/json'}
            response_pin = requests.request("PUT", "https://iij0y5sisb.execute-api.us-east-1.amazonaws.com/Streams/streams/streaming-12f4a3e5b9c5-pin/record", headers = headers, data = pin_payload)
            response_geo = requests.request("PUT", "https://iij0y5sisb.execute-api.us-east-1.amazonaws.com/Streams/streams/streaming-12f4a3e5b9c5-geo/record", headers = headers, data = geo_payload)   
            response_user = requests.request("PUT", "https://iij0y5sisb.execute-api.us-east-1.amazonaws.com/Streams/streams/streaming-12f4a3e5b9c5-user/record", headers = headers, data = user_payload)    
    ```

4. Read data into Databricks
    - With Acess Key and Encoded Acess Key, dataframes can be created to each specific data streams.
    ```
    #Pin Data
    df_pin = spark \
    .readStream \
    .format('kinesis') \
    .option('streamName','streaming-12f4a3e5b9c5-pin') \
    .option('initialPosition','earliest') \
    .option('region','us-east-1') \
    .option('awsAccessKey', ACCESS_KEY) \
    .option('awsSecretKey', SECRET_KEY) \
    .load()
    ```

    ```
    #Geo Data
    df_geo = spark \
    .readStream \
    .format('kinesis') \
    .option('streamName','streaming-12f4a3e5b9c5-geo') \
    .option('initialPosition','earliest') \
    .option('region','us-east-1') \
    .option('awsAccessKey', ACCESS_KEY) \
    .option('awsSecretKey', SECRET_KEY) \
    .load()
    ```

    ```
    #User Data
    df_user = spark \
    .readStream \
    .format('kinesis') \
    .option('streamName','streaming-12f4a3e5b9c5-user') \
    .option('initialPosition','earliest') \
    .option('region','us-east-1') \
    .option('awsAccessKey', ACCESS_KEY) \
    .option('awsSecretKey', SECRET_KEY) \
    .load()
    ```

5. Transform data
    - Before the dataframes can be clean, each dataframe's scheme needs to be defined.
    ```
    pin_schema = StructType([
    StructField("index", IntegerType()),
    StructField("unique_id", StringType()),
    StructField("title", StringType()),
    StructField("description", StringType()),
    StructField("poster_name", StringType()),
    StructField("follower_count", StringType()),
    StructField("tag_list", StringType()),
    StructField("is_image_or_video", StringType()),
    StructField("image_src", StringType()),
    StructField("downloaded", IntegerType()),
    StructField("save_location", StringType()),
    StructField("category", StringType())
    ])
    geo_schema = StructType([
    StructField("ind", IntegerType()),
    StructField("timestamp", TimestampType()),
    StructField("latitude", DoubleType()),
    StructField("longitude", DoubleType()),
    StructField("country", StringType())
    ])
    user_schema = StructType([
    StructField("ind", IntegerType()),
    StructField("first_name", StringType()),
    StructField("last_name", StringType()),
    StructField("age", StringType()),
    StructField("date_joined", TimestampType())
    ])
    ```

    - Each dataframe then needs to be deserialized 
    ```
    deserialize_df_pin = df_pin.selectExpr("CAST(data as STRING)")
    deserialize_df_pin = deserialize_df_pin.withColumn("data", from_json(col("data"), pin_schema))
    deserialize_df_pin = deserialize_df_pin.selectExpr("data.*")

    deserialize_df_geo = df_geo.selectExpr("CAST(data as STRING)")
    deserialize_df_geo = deserialize_df_geo.withColumn("data", from_json(col("data"), geo_schema))
    deserialize_df_geo = deserialize_df_geo.selectExpr("data.*")


    deserialize_df_user = df_user.selectExpr("CAST(data as STRING)")
    deserialize_df_user = deserialize_df_user.withColumn("data", from_json(col("data"), user_schema))
    deserialize_df_user = deserialize_df_user.selectExpr("data.*")
    ```

    - The data can then be clean the same way as we cleaned it on Milestone 7

6. Write the data to Delta Tables
    - Before it can be send into the delta tables, the checkpoint folder needs to be deleted.
    ```
    dbutils.fs.rm("/tmp/kinesis/_checkpoints/", True)
    ```

    - Then a clean dataframe can be written inside Delta Tables
    ```
    #Writes df_pin_clean dataframe into delta tables
    df_pin_clean.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/tmp/kinesis/_checkpoints/") \
    .table("12f4a3e5b9c5_pin_table")
    ```
