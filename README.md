# Pinterest Data Pipeline

## Table of Contents
- Project Brief
- Project Data
- Project Tools
- Milestone 3
- Milestone 4
- Milestone 5
- Milestone 6
- Milestone 7
- Milestone 8
- Milestone 9

## Project Brief
Build the system that Pinterest uses to analyse both historical, and real-time data generated by post from their users.

Pinterest has world-class machine learning engineering systems. They have billioons of user interactions such as image uploads or image clicks which they need to process every day to inform the decisions to make. In this project, I am building a system in the cloud that takes in those events and runs them throught two separate pipelines. One for computing real-time metrics such as profile popularity, which would be used to recommend that profile in real-time. Another is for computing metrics that depend on historical data such as the most popular category this year.

## Milestone 3: Configure the EC2 Kafka Client
Goal: 
- Create 3 topics with MSK

Tasks:
- Creating a .pem key
    - Navigating to AWS Parameter Store to find the key pair value and saving it as "Key pair name.pem" file.
- Connecting to EC2
    - Connecting my local device into AWS EC2 by running the code below inside a terminal:
    '''
    ssh -i "12f4a3e5b9c5-key-pair.pem" ec2-user@ec2-184-73-115-68.compute-1.amazonaws.com

    '''
- Set up Kafka in EC2 instance
    - Once the EC2 was connected, Kafka was installed and client.properties file was created to configure Kafka Client to use AWS IAM authentication to the cluster.

    client.properties file contains:
    '''
    # Sets up TLS for encryption and SASL for authN.
    security.protocol = SASL_SSL

    # Identifies the SASL mechanism to use.
    sasl.mechanism = AWS_MSK_IAM

    # Binds SASL client implementation.
    sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn="arn:aws:iam::584739742957:role/12f4a3e5b9c5-ec2-access-role";

    # Encapsulates constructing a SigV4 signature based on extracted credentials.
    # The SASL client bound by "sasl.jaas.config" invokes this class.
    sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler
    '''
- Creating Kafka Topics
    - Before a topic was created, I had to grabbed my MSK cluster specific Bootstrap servers string and also its Plaintext Apache Zookeeper connection string.
    - Using the correct strings, I managed to create 3 topics:
        - 12f4a3e5b9c5.pin
        - 12f4a3e5b9c5.geo
        - 12f4a3e5b9c5.user
    - The topics were created using this code below:
    '''
    ./kafka-topics.sh --create --zookeeper "PLAINTEXT://b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098
" --replication-factor 2 --partitions 1 --topic 12f4a3e5b9c5.pin
    '''

## Milestone 4: Connect a MSK cluster to a S3 bucket
Goal:
- Use MSK Connect to connect the MSK cluster to a S3 bucket, such that any data going through the cluster will be automatically saved and stored in a dedicated S3 bucket.

Tasks:
- Creating a custom plugin with MSK Connect
- Creating a connector with MSK Connect

## Milestone 5: Configuring an API Gateway
Goal: 
- To build our own API that send data to the MSK cluter, which in turn will be stored in an S3 bucket.

Tasks:
- Build a Kafka REST Proxy
- Set up the REST Proxy inside EC2 client
- Send data to the API

## Milestone 6: Set-up Databricks
Goal:
- To mount S3 bucket inside Databricks

Tasks:
- Mount S3 bucket into Databricks
- Create a dataframe for each topic

## Milestone 7: Spark usage in Databricks
Goal:
- To clean and perform analyses to the data

Tasks:
- Clean each dataframe
- Query the data
    - Find the most popular category in each country
    - Find which was the most popular category each year
    - Find the user with the most followers in each country
    - Find the most popular category for different age groups
    - Find the median follower count for different age groups
    - Find how many users have joined each year
    - Find the median follower count of user based on their joining year
    - Find the median follower count of used based on their joining year and age group

## Milestone 8: AWS MWAA
Goal:
- Orchestrate Databricks workloads on AWS MWAA

Tasks
- Create and upload a DAG file into MWAA environment
- Manually trigger the DAG to run a Databricks Notebook

## Milestone 9: AWS Kinesis
Goal:
- Send streaming data to Kinesis and read this data inside Databricks

Tasks
- Create data stream using AWS Kinesis
- Configure API with Kinesis proxy integration
- Send data into Kinesis streams
- Read data into Databricks
- Transform data
- Write the data to Delta Tables